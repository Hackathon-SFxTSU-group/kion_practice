import os
import cv2
import json
import time
from tqdm import tqdm

from scene_splitter.clip_scene_splitter import CLIPSceneSplitter
from tracking.detector import ObjectDetector
from tracking.tracker import DeepSortTracker
# from tracking.face_reid_pytorch import FaceReId  # –†–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

class VideoPipeline:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Å—Ü–µ–Ω–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ: –≤—ã–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω, —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏—Ü),
    —Å–±–æ—Ä –≤—ã—Ä–µ–∑–∞–Ω–Ω—ã—Ö –ª–∏—Ü –∏ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç —Ç—Ä–µ–∫–æ–≤. –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ JSON-–æ—Ç—á–µ—Ç–æ–≤ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.
    """

    def __init__(self, using_cache=False, render_video=False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≤–∏–¥–µ–æ–ø–∞–π–ø–ª–∞–π–Ω–∞.

        :param using_cache: –µ—Å–ª–∏ True, –Ω–µ —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        :param render_video: –µ—Å–ª–∏ True, –≤–∫–ª—é—á–∞–µ—Ç—Å—è —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ
        """
        self.render_video = render_video

        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–∞–π–ø–ª–∞–π–Ω–∞, —Å–æ–∑–¥–∞—é—Ç—Å—è, –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à
        if not using_cache:
            self.splitter = CLIPSceneSplitter()       # –í—ã–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP
            self.detector = ObjectDetector()          # –î–µ—Ç–µ–∫—Ç–æ—Ä –æ–±—ä–µ–∫—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏—Ü)
            self.tracker = DeepSortTracker()          # DeepSORT-—Ç—Ä–µ–∫–µ—Ä –æ–±—ä–µ–∫—Ç–æ–≤
            # self.face_reid = FaceReId()              # –†–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

        # –°–ª—É–∂–µ–±–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.scene_data = []          # –°–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω —Å —Ç—Ä–µ–∫-–¥–∞–Ω–Ω—ã–º–∏
        self.track_faces = {}         # –í—ã—Ä–µ–∑–∞–Ω–Ω—ã–µ –ª–∏—Ü–∞ –ø–æ track_id
        self.tracking_frames = {}     # –°–ª–æ–≤–∞—Ä—å: track_id -> {frame_idx: (x1, y1, x2, y2)}

    def run(self, video_path, output_dir="output_files"):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ: —Å—Ü–µ–Ω–æ–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ, —Ç—Ä–µ–∫–∏–Ω–≥, —ç–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞.

        :param video_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ—Ñ–∞–π–ª—É
        :param output_dir: –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        :return: –ö–æ—Ä—Ç–µ–∂: (scene_data, track_faces, tracking_frames)
        """
        os.makedirs(output_dir, exist_ok=True)
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_video_path = os.path.join(output_dir, f"{video_name}_annotated.mp4")

        # === –®–∞–≥ 1: –°—Ü–µ–Ω–æ–≤–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ ===
        print("üé¨ –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ü–µ–Ω...")
        t0 = time.time()
        scenes = self.splitter.detect_scenes(video_path)  # [(start, end), ...]
        scene_split_time = time.time() - t0
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(scenes)} —Å—Ü–µ–Ω –∑–∞ {scene_split_time:.2f} —Å–µ–∫")

        # === –®–∞–≥ 2: –¢—Ä–µ–∫–∏–Ω–≥ –ø–æ —Å—Ü–µ–Ω–∞–º ===
        print("üß† –ê–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤...")
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)

        t1 = time.time()
        for (start, end) in tqdm(scenes, desc="Analyzing scenes"):
            cap.set(cv2.CAP_PROP_POS_MSEC, start * 1000)  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
            frame_idx = int(start * fps)
            end_idx = int(end * fps)
            track_ids_in_scene = set()

            while frame_idx < end_idx:
                ret, frame = cap.read()
                if not ret:
                    break

                # –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤
                detections = self.detector.detect(frame)
                tracks = self.tracker.update(detections, frame)

                for t in tracks:
                    track_id = t['track_id']
                    x1, y1, x2, y2 = map(int, t['bbox'])
                    track_ids_in_scene.add(track_id)

                    # –í—ã—Ä–µ–∑–∞–µ–º –ª–∏—Ü–æ
                    face_crop = frame[y1:y2, x1:x2]
                    if face_crop.size == 0:
                        continue

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∏—Ü–æ –ø–æ track_id
                    self.track_faces.setdefault(track_id, []).append(face_crop)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ –∫–∞–¥—Ä–∞–º
                    self.tracking_frames.setdefault(track_id, {})[frame_idx] = (x1, y1, x2, y2)

                frame_idx += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ü–µ–Ω—É –∏ –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ç—Ä–µ–∫–∏ –≤ –Ω–µ–π
            self.scene_data.append({
                "start": start,
                "end": end,
                "track_ids": list(track_ids_in_scene)
            })

        tracking_time = time.time() - t1
        cap.release()

        # === –®–∞–≥ 3: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON-–æ—Ç—á–µ—Ç–∞ ===
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ JSON –æ—Ç—á–µ—Ç–∞...")
        self.export_report_to_json(video_path, output_dir, scene_split_time, tracking_time)

        # === –®–∞–≥ 4: (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –†–µ–Ω–¥–µ—Ä –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ ===
        # track_id_to_person = {track_id: f"person_{track_id}" for track_id in self.track_faces}
        # if self.render_video:
        #     print("üé• –†–µ–Ω–¥–µ—Ä –≤–∏–¥–µ–æ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏...")
        #     self.render_labeled_video(video_path, output_video_path, self.tracking_frames, track_id_to_person)

        print("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω.")
        return self.scene_data, self.track_faces, self.tracking_frames

    def export_report_to_json(self, video_path, output_dir, scene_split_time, tracking_time):
        """
        –§–æ—Ä–º–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç JSON-–æ—Ç—á–µ—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å—Ü–µ–Ω–∞—Ö –∏ —Ç—Ä–µ–∫–∞—Ö.

        :param video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ
        :param output_dir: –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        :param scene_split_time: –í—Ä–µ–º—è –Ω–∞ —Å—Ü–µ–Ω–æ–≤—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä
        :param tracking_time: –í—Ä–µ–º—è –Ω–∞ —Ç—Ä–µ–∫–∏–Ω–≥
        """
        report = {
            "video_path": video_path,
            "scene_data": self.scene_data,
            "track_faces": {str(k): len(v) for k, v in self.track_faces.items()},
            "tracking_frames": {str(k): list(v.keys()) for k, v in self.tracking_frames.items()},
            "timing": {
                "scene_split_time_sec": round(scene_split_time, 3),
                "tracking_time_sec": round(tracking_time, 3)
            }
        }

        os.makedirs("reports", exist_ok=True)
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_path = os.path.join("reports", f"{video_name}_video_report.json")

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=4, ensure_ascii=False)

        print(f"üìÑ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

    def render_labeled_video(self, video_path, output_path, tracking_frames, track_id_to_person):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–¥–µ–æ —Å –æ—Ç—Ä–∏—Å–æ–≤–∞–Ω–Ω—ã–º–∏ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞–º–∏ –∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –ø–æ track_id.

        :param video_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
        :param output_path: –ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ
        :param tracking_frames: –¢—Ä–µ–∫–∏ —Å –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º–∏ bbox –ø–æ –∫–∞–¥—Ä–∞–º
        :param track_id_to_person: –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–∫–æ–≤ –∏ –∏–º—ë–Ω
        """
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            for track_id, frames in tracking_frames.items():
                if frame_idx in frames:
                    x1, y1, x2, y2 = frames[frame_idx]
                    name = track_id_to_person.get(track_id, f"person_{track_id}")
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, name, (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            out.write(frame)
            frame_idx += 1

        cap.release()
        out.release()
        print(f"üìº –†–∞–∑–º–µ—á–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
