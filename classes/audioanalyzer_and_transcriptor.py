import os
import json
import torch
import whisper
import librosa
import torchaudio
import torchopenl3
import numpy as np
import soundfile as sf
import tensorflow_hub as hub
from scipy.ndimage import gaussian_filter1d
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
from utils.audio_tools import extract_audio_ffmpeg


class AudioAnalyzer:
    def __init__(self, video_path, output_dir="temp", whisper_model_size="tiny",
                 ffmpeg_path="ffmpeg", ffprobe_path="ffprobe", device="cuda"):
        self.video_path = video_path
        self.output_dir = output_dir
        self.audio_path = os.path.join(output_dir, "temp_audio.wav")
        self.ffmpeg_path = ffmpeg_path
        self.ffprobe_path = ffprobe_path
        self.device = device

        os.makedirs(self.output_dir, exist_ok=True)

        self.openl3_model = None
        self.yamnet_model = None
        self.whisper_model = whisper.load_model(whisper_model_size, device=self.device)

        self.segments = []
        self.segments_short = []

    def extract_audio(self):
        print("üéß –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ...")
        extract_audio_ffmpeg(self.video_path, self.audio_path, ffmpeg_path=self.ffmpeg_path)
        print(f"‚úÖ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.audio_path}")

    def load_models(self):
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ torchopenl3...")
        if self.openl3_model is None:
            self.openl3_model = torchopenl3.models.load_audio_embedding_model(
                input_repr="mel256", content_type="music", embedding_size=6144
            ).to(self.device)
        print("‚úÖ torchopenl3 –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

        if self.yamnet_model is None:
            self.yamnet_model = hub.load("https://tfhub.dev/google/yamnet/1")
            print("‚úÖ YAMNet –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    def extract_openl3_features(self):
        print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø–æ–º–æ—â—å—é torchopenl3...")

        # –°–æ–∑–¥–∞—ë–º –∫–æ–ø–∏—é .wav —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è openl3
        openl3_audio_path = os.path.join(self.output_dir, "temp_audio_openl3.wav")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –ø–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ float32 PCM
        audio, sr = sf.read(self.audio_path)
        if len(audio.shape) > 1:
            audio = np.mean(audio, axis=1)
        audio = audio.astype(np.float32)
        sf.write(openl3_audio_path, audio, sr, subtype="PCM_16")

        # –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —á—Ç–µ–Ω–∏–µ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
        audio, sr = sf.read(openl3_audio_path)
        audio = audio.astype(np.float32)

        print(f"‚è± –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞—É–¥–∏–æ: {len(audio) / sr:.2f} —Å–µ–∫")

        embeddings, timestamps = torchopenl3.get_audio_embedding(
            audio,
            sr,
            model=self.openl3_model,
            hop_size=1.0,
            center=True
        )

        embeddings = embeddings.squeeze(0)  # (1, T, D) ‚Üí (T, D)

        print(f"‚úÖ torchopenl3 –≤–µ—Ä–Ω—É–ª {embeddings.shape[0]} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.")

        if embeddings.shape[0] > 1:
            embeddings = (embeddings - embeddings.mean(dim=0)) / embeddings.std(dim=0)
        else:
            print("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ‚Äî —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥.")

        return embeddings.cpu().numpy(), timestamps

    def extract_yamnet_labels(self):
        print("üîä –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–∫ —Å YAMNet...")
        waveform, sr = librosa.load(self.audio_path, sr=16000, mono=True)
        waveform = waveform.astype(np.float32)

        scores, embeddings, spectrogram = self.yamnet_model(waveform)

        try:
            predictions = np.argmax(scores.numpy(), axis=1)
        except Exception as e:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ YAMNet:", e)
            predictions = []

        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(predictions)} –∞—É–¥–∏–æ–º–µ—Ç–æ–∫.")
        return predictions.tolist()

    def compute_rms(self, frame_duration=1.0):
        print("üìä –†–∞—Å—á—ë—Ç RMS-–≥—Ä–æ–º–∫–æ—Å—Ç–∏...")
        y, sr = librosa.load(self.audio_path, sr=None)
        frame_length = int(sr * frame_duration)
        hop_length = frame_length
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=hop_length)
        print(f"‚úÖ RMS —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –¥–ª—è {len(rms)} –æ–∫–æ–Ω.")
        return times, rms

    def detect_scenes(self, features, timestamps, yamnet_labels, rms_t, rms_vals, sensitivity, min_scene_duration):
        print("‚úÇÔ∏è –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Å—Ü–µ–Ω...")
        if features.shape[0] < 2:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

        smoothed = gaussian_filter1d(features, sigma=2, axis=0)
        diffs = [
            1 - cosine_similarity([smoothed[i - 1]], [smoothed[i]])[0][0]
            for i in range(1, len(smoothed))
        ]
        diffs = np.array(diffs)

        threshold = np.percentile(diffs, 100 * sensitivity)
        change_points = [i for i, d in enumerate(diffs) if d > threshold]

        print(f"üü¢ –ù–∞–π–¥–µ–Ω–æ {len(change_points)} –ø–µ—Ä–µ—Ö–æ–¥–æ–≤ —Å—Ü–µ–Ω.")
        return change_points

    def process_asr(self, language="ru"):
        print("üó£ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å Whisper...")
        result = self.whisper_model.transcribe(self.audio_path, language=language)
        self.segments = result["segments"]
        self.segments_short = [
            {"start": seg["start"], "end": seg["end"], "text": seg["text"]}
            for seg in result["segments"]
        ]

    def run(self, sensitivity=0.85, min_scene_duration=2.0):
        print("üöÄ –ó–∞–ø—É—Å–∫ –∞—É–¥–∏–æ–∞–Ω–∞–ª–∏–∑–∞...")
        self.extract_audio()
        self.load_models()

        features, timestamps = self.extract_openl3_features()
        yamnet_labels = self.extract_yamnet_labels()
        rms_t, rms_vals = self.compute_rms()

        changes = self.detect_scenes(features, timestamps, yamnet_labels, rms_t, rms_vals,
                                     sensitivity, min_scene_duration)

        self.process_asr(language="ru")
        print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω.")

    def detect_audio_activity(self, frame_duration=1.0):
        _, rms_vals = self.compute_rms(frame_duration)
        return rms_vals.tolist()
