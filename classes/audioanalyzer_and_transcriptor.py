import os
import json
import torch
import whisper
import librosa
import openl3
import numpy as np
import tensorflow_hub as hub
from tqdm import tqdm
from datetime import datetime
from scipy.ndimage import gaussian_filter1d
from sklearn.metrics.pairwise import cosine_similarity

from utils.audio_tools import extract_audio_ffmpeg, get_video_duration


class AudioAnalyzer:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–∏ –≤–∏–¥–µ–æ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–µ–π:
    - OpenL3 (–∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤),
    - YAMNet (–∞—É–¥–∏–æ–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è),
    - Whisper (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏),
    - RMS (—ç–Ω–µ—Ä–≥–∏—è —Å–∏–≥–Ω–∞–ª–∞).
    """

    def __init__(self, video_path, output_dir="output_scenes", whisper_model_size="tiny",
                 ffmpeg_path="ffmpeg", ffprobe_path="ffprobe"):
        # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        self.video_path = video_path
        self.output_dir = output_dir
        self.audio_path = os.path.join(output_dir, "temp_audio.wav")
        self.ffmpeg_path = ffmpeg_path
        self.ffprobe_path = ffprobe_path

        # –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–∏–¥–µ–æ
        self.video_duration = 0

        # –ú–æ–¥–µ–ª–∏ (–∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ–∑–∂–µ)
        self.openl3_model = None
        self.yamnet_model = None

        # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∏ –ª–æ–≥–æ–≤
        self.segments = []           # –ü–æ–ª–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã –∏–∑ Whisper
        self.segments_short = []     # –£–∫–æ—Ä–æ—á–µ–Ω–Ω—ã–µ {start, end, text}
        self.detection_log = []      # –õ–æ–≥ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –¥–ª—è —Å—Ü–µ–Ω

        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ –º–æ–¥–µ–ª—å Whisper
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.whisper_model = whisper.load_model(whisper_model_size, device=self.device)

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        os.makedirs(output_dir, exist_ok=True)
        os.makedirs("reports", exist_ok=True)

    def extract_audio(self):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫—É –∏–∑ –≤–∏–¥–µ–æ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –µ—ë –≤ WAV-—Ñ–∞–π–ª.
        """
        print("üéß –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ –∏–∑ –≤–∏–¥–µ–æ...")
        self.audio_path = extract_audio_ffmpeg(self.video_path, self.audio_path, self.ffmpeg_path)
        self.video_duration = get_video_duration(self.video_path, self.ffprobe_path)
        print(f"‚úÖ –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.audio_path}")

    def load_models(self):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏ OpenL3 –∏ YAMNet (–µ—Å–ª–∏ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã).
        """
        print("üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π OpenL3 –∏ YAMNet...")
        if self.openl3_model is None:
            self.openl3_model = openl3.models.load_audio_embedding_model("mel256", "music", 6144)
        if self.yamnet_model is None:
            self.yamnet_model = hub.load("https://tfhub.dev/google/yamnet/1")
        print("‚úÖ –ú–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")

    def extract_openl3_features(self):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∞—É–¥–∏–æ—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ OpenL3 –ø–æ –≤—Å–µ–π –∞—É–¥–∏–æ–¥–æ—Ä–æ–∂–∫–µ.
        """
        print("üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ–ø—Ä–∏–∑–Ω–∞–∫–æ–≤ OpenL3...")
        y, sr = librosa.load(self.audio_path, sr=44100, mono=True)
        features, timestamps = openl3.get_audio_embedding(y, sr, model=self.openl3_model, hop_size=1.0)
        features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)  # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(features)} –≤–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
        return features, timestamps

    def extract_yamnet_labels(self):
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ—Å–∏–≥–Ω–∞–ª –ø–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–º –æ—Ç—Ä–µ–∑–∫–∞–º —Å –ø–æ–º–æ—â—å—é YAMNet.
        """
        print("üîä –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–∫ —Å YAMNet...")
        waveform, sr = librosa.load(self.audio_path, sr=16000)
        yamnet_output = self.yamnet_model(waveform)
        scores = yamnet_output[0].numpy()
        labels = np.argmax(scores, axis=1)
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(labels)} –∞—É–¥–∏–æ–º–µ—Ç–æ–∫.")
        return labels

    def compute_rms(self):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç —É—Ä–æ–≤–µ–Ω—å RMS (—ç–Ω–µ—Ä–≥–∏–∏ —Å–∏–≥–Ω–∞–ª–∞) –ø–æ –≤—Ä–µ–º–µ–Ω–∏.
        """
        print("üìä –†–∞—Å—á—ë—Ç RMS-–≥—Ä–æ–º–∫–æ—Å—Ç–∏...")
        y, sr = librosa.load(self.audio_path, sr=44100)
        rms = librosa.feature.rms(y=y)[0]
        times = librosa.frames_to_time(np.arange(len(rms)), sr=sr)
        print(f"‚úÖ RMS —Ä–∞—Å—Å—á–∏—Ç–∞–Ω–æ –¥–ª—è {len(rms)} –æ–∫–æ–Ω.")
        return times, rms

    def detect_scenes(self, features, timestamps, yamnet_labels, rms_t, rms_vals,
                      sensitivity=0.85, min_scene_duration=2.0):
        """
        –í—ã—è–≤–ª—è–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å—Ü–µ–Ω —Å –ø–æ–º–æ—â—å—é –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏—è –ø–æ —Ç—Ä–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
        - –∏–∑–º–µ–Ω–µ–Ω–∏–µ OpenL3 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,
        - —Å–º–µ–Ω–∞ –∫–ª–∞—Å—Å–∞ YAMNet,
        - —Å–∫–∞—á–æ–∫ RMS-–≥—Ä–æ–º–∫–æ—Å—Ç–∏.
        """
        print("‚úÇÔ∏è –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü —Å—Ü–µ–Ω...")
        smoothed = gaussian_filter1d(features, sigma=2, axis=0)
        diffs = [
            1 - cosine_similarity([smoothed[i - 1]], [smoothed[i]])[0][0]
            for i in tqdm(range(1, len(smoothed)), desc="üõë –ê–Ω–∞–ª–∏–∑")
        ]
        threshold = np.percentile(diffs, sensitivity * 100)

        changes = []
        prev_time = -min_scene_duration
        self.detection_log = []

        for i, diff in enumerate(diffs):
            t = timestamps[i + 1]
            audio_change = diff > threshold
            yamnet_change = (yamnet_labels[i] != yamnet_labels[i + 1])
            rms_change = np.abs(rms_vals[min(i, len(rms_vals) - 1)] - rms_vals[max(i - 1, 0)]) > 0.1

            vote = sum([audio_change, yamnet_change, rms_change]) >= 2
            self.detection_log.append((t, audio_change, yamnet_change, rms_change, vote))

            if vote and (t - prev_time >= min_scene_duration):
                changes.append(t)
                prev_time = t

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ü–µ–Ω: {len(changes)}")
        return changes

    def process_asr(self, language="ru"):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é Whisper.
        """
        print("üìù –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ (Whisper)...")
        result = self.whisper_model.transcribe(
            self.audio_path,
            language=language,
            verbose=True,
            word_timestamps=True
        )
        self.segments = result["segments"]
        self.segments_short = [
            {"start": round(seg["start"], 2), "end": round(seg["end"], 2), "text": seg["text"]}
            for seg in self.segments
        ]
        print(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(self.segments)}")

    def detect_audio_activity(self, frame_duration=1.0):
        """
        –í—ã—á–∏—Å–ª—è–µ—Ç RMS-—ç–Ω–µ—Ä–≥–∏—é –¥–ª—è —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω (–∫–∞–¥—Ä–æ–≤).
        """
        print(f"üéõ –†–∞—Å—á—ë—Ç –∞—É–¥–∏–æ—ç–Ω–µ—Ä–≥–∏–∏ –ø–æ –æ–∫–Ω–∞–º –ø–æ {frame_duration} —Å–µ–∫...")
        y, sr = librosa.load(self.audio_path, sr=None)
        frame_len = int(sr * frame_duration)
        energy = [
            np.sqrt(np.mean(y[i:i + frame_len] ** 2))
            for i in range(0, len(y), frame_len)
        ]
        print(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(energy)} –æ–∫–æ–Ω.")
        return energy

    def export_full_report(self, changes, segments_short, sensitivity=0.85, min_scene_duration=2.0):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á—ë—Ç –æ–± –∞—É–¥–∏–æ–∞–Ω–∞–ª–∏–∑–µ, –≤–∫–ª—é—á–∞—è —Å—Ü–µ–Ω—ã, —Ä–µ—á—å –∏ –ª–æ–≥–∏.
        """
        print("üíæ –≠–∫—Å–ø–æ—Ä—Ç –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...")
        scene_bounds = [0] + changes + [self.video_duration]
        scenes = [{
            "scene_id": i + 1,
            "start": round(scene_bounds[i], 3),
            "end": round(scene_bounds[i + 1], 3),
            "duration": round(scene_bounds[i + 1] - scene_bounds[i], 3)
        } for i in range(len(scene_bounds) - 1)]

        detection_log = [{
            "time": round(t, 3),
            "audio_change": bool(a),
            "yamnet_change": bool(y),
            "rms_change": bool(r),
            "voted": bool(v)
        } for t, a, y, r, v in self.detection_log]

        total_duration = sum(round(seg["end"] - seg["start"], 2) for seg in self.segments)
        total_words = sum(len(seg["text"].split()) for seg in self.segments)

        segment_reports = [{
            "start": round(seg["start"], 2),
            "end": round(seg["end"], 2),
            "duration": round(seg["end"] - seg["start"], 2),
            "word_count": len(seg["text"].split()),
            "text": seg["text"].strip()
        } for seg in self.segments]

        asr_summary = {
            "total_segments": len(self.segments),
            "total_duration_sec": round(total_duration, 2),
            "avg_segment_duration_sec": round(total_duration / len(self.segments), 2) if self.segments else 0,
            "avg_word_count_per_segment": round(total_words / len(self.segments), 2) if self.segments else 0
        }

        report_data = {
            "video_path": self.video_path,
            "method": "audio_openl3_yamnet_rms_whisper",
            "generated_at": datetime.now().isoformat(),
            "sensitivity": sensitivity,
            "min_scene_duration": min_scene_duration,
            "scene_count": len(scenes),
            "asr_summary": asr_summary,
            "scenes": scenes,
            "speech_segments": segments_short,
            "speech_segments_detailed": segment_reports,
            "detection_log": detection_log,
            "model_used": ["openl3", "yamnet", "rms", "whisper"]
        }

        filename = os.path.splitext(os.path.basename(self.video_path))[0]
        path = os.path.join("reports", f"{filename}_full_report.json")
        with open(path, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {path}")

    def run(self, sensitivity=0.85, min_scene_duration=2.0):
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –∞—É–¥–∏–æ–∞–Ω–∞–ª–∏–∑–∞:
        - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ,
        - –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π,
        - –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤,
        - —Å—Ü–µ–Ω–æ–≤–æ–µ –¥–µ–ª–µ–Ω–∏–µ,
        - —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏,
        - —ç–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞.
        """
        self.extract_audio()
        self.load_models()
        features, timestamps = self.extract_openl3_features()
        yamnet_labels = self.extract_yamnet_labels()
        rms_t, rms_vals = self.compute_rms()
        changes = self.detect_scenes(features, timestamps, yamnet_labels, rms_t, rms_vals,
                                     sensitivity, min_scene_duration)
        self.process_asr(language="ru")
        self.export_full_report(changes, self.segments_short, sensitivity, min_scene_duration)
        print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ: {len(changes) + 1} —Å—Ü–µ–Ω, {len(self.segments_short)} —Ä–µ—á–µ–≤—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤.")
