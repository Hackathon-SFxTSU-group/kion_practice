import os
import cv2
import json
import time
from tqdm import tqdm

from scene_splitter.clip_scene_splitter import CLIPSceneSplitter  # –°—Ü–µ–Ω–æ–≤—ã–π —Å–ø–ª–∏—Ç—Ç–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ CLIP
from tracking.detector import ObjectDetector                       # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ª–∏—Ü)
from tracking.tracker import DeepSortTracker                      # –¢—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ –≤–∏–¥–µ–æ
from tracking.face_reid import FaceReId                           # –†–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü –ø–æ embedding'–∞–º


class VideoPipeline:
    """
    –ö–ª–∞—Å—Å VideoPipeline —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ø–∞–π–ø–ª–∞–π–Ω:
    - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –Ω–∞ —Å—Ü–µ–Ω—ã,
    - –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤,
    - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª–∏—Ü –ø–æ track_id,
    - –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) —Ä–µ–Ω–¥–µ—Ä —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ.
    """

    def __init__(self, using_cache=False, render_video=False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –ø–∞–π–ø–ª–∞–π–Ω–∞.

        :param using_cache: –µ—Å–ª–∏ True ‚Äî –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–Ω–æ
        :param render_video: –µ—Å–ª–∏ True ‚Äî —Å–æ–∑–¥–∞—ë—Ç—Å—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ —Å ID –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
        """
        self.render_video = render_video

        if not using_cache:
            self.splitter = CLIPSceneSplitter()    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤–∏–¥–µ–æ –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ü–µ–Ω—ã
            self.detector = ObjectDetector()       # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤ (–ª–∏—Ü)
            self.tracker = DeepSortTracker()       # –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤
            self.face_reid = FaceReId()            # –†–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü

        # –•—Ä–∞–Ω–∏–ª–∏—â–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞
        self.scene_data = []          # –î–∞–Ω–Ω—ã–µ –ø–æ —Å—Ü–µ–Ω–∞–º –∏ track_id
        self.track_faces = {}         # –õ–∏—Ü–∞ –ø–æ track_id (–¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)
        self.tracking_frames = {}     # –ü—Ä–∏–≤—è–∑–∫–∞ —Ç—Ä–µ–∫–æ–≤ –∫ –∫–∞–¥—Ä–∞–º

    def run(self, video_path, output_dir="output_files"):
        """
        –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞: —Å—Ü–µ–Ω–æ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ, —Ç—Ä–µ–∫–∏–Ω–≥, —ç–∫—Å–ø–æ—Ä—Ç –æ—Ç—á–µ—Ç–∞ –∏ —Ä–µ–Ω–¥–µ—Ä.

        :param video_path: –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—É
        :param output_dir: –ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        :return: –ö–æ—Ä—Ç–µ–∂ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: scene_data, track_faces, tracking_frames, track_id_to_person
        """
        os.makedirs(output_dir, exist_ok=True)
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_video_path = os.path.join(output_dir, f"{video_name}_annotated.mp4")

        # === –®–∞–≥ 1: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ —Å—Ü–µ–Ω—ã ===
        print("üé¨ –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ü–µ–Ω...")
        t0 = time.time()
        scenes = self.splitter.detect_scenes(video_path)
        scene_split_time = time.time() - t0
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å—Ü–µ–Ω: {len(scenes)} –∑–∞ {scene_split_time:.2f} —Å–µ–∫")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤–∏–¥–µ–æ
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)

        # === –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π —Å—Ü–µ–Ω—ã ===
        print("üß† –ê–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤...")
        t1 = time.time()
        for (start, end) in tqdm(scenes, desc="Analyzing scenes"):
            cap.set(cv2.CAP_PROP_POS_MSEC, start * 1000)
            frame_idx = int(start * fps)
            end_idx = int(end * fps)
            track_ids_in_scene = set()

            while frame_idx < end_idx:
                ret, frame = cap.read()
                if not ret:
                    break

                # –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –∫–∞–¥—Ä–µ
                detections = self.detector.detect(frame)
                tracks = self.tracker.update(detections, frame)

                for t in tracks:
                    track_id = t['track_id']
                    x1, y1, x2, y2 = map(int, t['bbox'])
                    track_ids_in_scene.add(track_id)

                    # –í—ã—Ä–µ–∑–∞–µ–º –ª–∏—Ü–æ
                    face_crop = frame[y1:y2, x1:x2]
                    if face_crop.size == 0:
                        continue

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∏—Ü–æ –ø–æ track_id
                    self.track_faces.setdefault(track_id, []).append(face_crop)

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º bbox –≤ –Ω—É–∂–Ω—ã–π –∫–∞–¥—Ä
                    self.tracking_frames.setdefault(track_id, {})[frame_idx] = (x1, y1, x2, y2)

                frame_idx += 1

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ü–µ–Ω—É —Å track_id –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π
            self.scene_data.append({
                "start": start,
                "end": end,
                "track_ids": list(track_ids_in_scene)
            })

        tracking_time = time.time() - t1
        cap.release()

        # === –®–∞–≥ 3: –≠–∫—Å–ø–æ—Ä—Ç –æ—Ç—á—ë—Ç–∞ ===
        print("üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
        self.export_report_to_json(video_path, output_dir, scene_split_time, tracking_time)

        # === –®–∞–≥ 4: –†–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü ===
        print("üß¨ –†–µ–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ü...")
        track_id_to_person = self.face_reid.analyze_persons(self.track_faces)

        # === –®–∞–≥ 5: –†–µ–Ω–¥–µ—Ä –≤–∏–¥–µ–æ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ) ===
        if self.render_video:
            print("üé• –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ...")
            self.render_labeled_video(video_path, output_video_path, self.tracking_frames, track_id_to_person)

        print("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à—ë–Ω.")
        return self.scene_data, self.track_faces, self.tracking_frames, track_id_to_person

    def export_report_to_json(self, video_path, output_dir, scene_split_time, tracking_time):
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π JSON-–æ—Ç—á—ë—Ç –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞ –≤–∏–¥–µ–æ.

        :param video_path: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—É—Ç—å –∫ –≤–∏–¥–µ–æ
        :param output_dir: –ö–∞—Ç–∞–ª–æ–≥ –≤—ã–≤–æ–¥–∞
        :param scene_split_time: –í—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —Å—Ü–µ–Ω–æ–≤–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        :param tracking_time: –í—Ä–µ–º—è, –∑–∞—Ç—Ä–∞—á–µ–Ω–Ω–æ–µ –Ω–∞ —Ç—Ä–µ–∫–∏–Ω–≥
        """
        report = {
            "video_path": video_path,
            "scene_data": self.scene_data,
            "track_faces": {str(k): len(v) for k, v in self.track_faces.items()},
            "tracking_frames": {str(k): list(v.keys()) for k, v in self.tracking_frames.items()},
            "timing": {
                "scene_split_time_sec": round(scene_split_time, 3),
                "tracking_time_sec": round(tracking_time, 3)
            }
        }

        os.makedirs("reports", exist_ok=True)
        video_name = os.path.splitext(os.path.basename(video_path))[0]
        output_path = os.path.join("reports", f"{video_name}_video_report.json")

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=4, ensure_ascii=False)

        print(f"üìÑ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

    def render_labeled_video(self, video_path, output_path, tracking_frames, track_id_to_person):
        """
        –û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ —Å –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∞–º–∏ –≤–æ–∫—Ä—É–≥ –ª–∏—Ü –∏ –ø–æ–¥–ø–∏—Å—è–º–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π.

        :param video_path: –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É –≤–∏–¥–µ–æ
        :param output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–≥–æ –≤–∏–¥–µ–æ
        :param tracking_frames: –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —Ç—Ä–µ–∫–æ–≤ –ø–æ –∫–∞–¥—Ä–∞–º
        :param track_id_to_person: –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ track_id ‚Üí –∏–º—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞
        """
        cap = cv2.VideoCapture(video_path)
        fps = cap.get(cv2.CAP_PROP_FPS)
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        frame_idx = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            # –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∏ –∏ –∏–º–µ–Ω–∞
            for track_id, frames in tracking_frames.items():
                if frame_idx in frames:
                    x1, y1, x2, y2 = frames[frame_idx]
                    name = track_id_to_person.get(track_id, f"person_{track_id}")
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, name, (x1, y1 - 10),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

            out.write(frame)
            frame_idx += 1

        cap.release()
        out.release()
        print(f"üìº –†–∞–∑–º–µ—á–µ–Ω–Ω–æ–µ –≤–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")
