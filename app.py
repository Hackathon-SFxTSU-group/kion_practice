# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π
import os
import pickle
#from classes.video_analyzer_complex import VideoPipeline
from classes.video_anlyzer_without_faceraid import VideoPipeline
from classes.scene_enricher import SceneEnricher
from classes.openai import OpenAIThemer
from classes.simple_transcriptor import ASRProcessor
from classes.scene_merge import UnifiedSceneMerger
import matplotlib.pyplot as plt
from classes.audioanalyzer_and_transcriptor import AudioAnalyzer


VIDEO_PATH = "videos/Video_02.avi"          # –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—É
AUDIO_PATH = "temp/temp_audio.wav"           # –í—Ä–µ–º–µ–Ω–Ω—ã–π WAV
OUTPUT_DIR = "output_files/"
TEMP_DIR = "temp/"         # –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
CACHE_PATH = os.path.join(TEMP_DIR, "results.pkl")
MIN_SCENE_LENGTH = 2.0                  # –ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã
MAX_SCENE_LENGTH = 300.0                # –ú–∞–∫—Å. –¥–æ–ø—É—Å—Ç–∏–º–∞—è –¥–ª–∏–Ω–∞ —Å—Ü–µ–Ω—ã (5 –º–∏–Ω)

if os.path.exists(CACHE_PATH):
    print("üîÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫–µ—à–∞...")
    with open(CACHE_PATH, "rb") as f:
        #scenes, track_faces, tracking_frames, track_id_to_person = pickle.load(f)
        scenes, track_faces, tracking_frames = pickle.load(f)

else:
    pipeline = VideoPipeline()
    #scenes, track_faces, tracking_frames, track_id_to_person = pipeline.run(VIDEO_PATH, OUTPUT_DIR)
    scenes, track_faces, tracking_frames = pipeline.run(VIDEO_PATH, OUTPUT_DIR)

    with open(CACHE_PATH, "wb") as f:
        #pickle.dump((scenes, track_faces, tracking_frames, track_id_to_person), f)
        pickle.dump((scenes, track_faces, tracking_frames), f)

# –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞
splitter = AudioAnalyzer(
    video_path=VIDEO_PATH,
    output_dir=OUTPUT_DIR,
    whisper_model_size='small'
)
splitter.run(sensitivity=0.88, min_scene_duration=2.0)

segments_short = splitter.segments_short  # –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤ –∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–æ–∫
segments_full = splitter.segments   

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ—ç–Ω–µ—Ä–≥–∏–∏ –ø–æ –∫–∞–¥—Ä–∞–º
energy = splitter.detect_audio_activity(frame_duration=1.0)
enricher = SceneEnricher(segments_full, energy)
#scenes_final = enricher.run(scenes, track_id_to_person)
scenes_final = enricher.run(scenes)
#scenes_final = enricher.run(scene_data, track_id_to_person, print_report=False)

themer = OpenAIThemer(
        api_key="",  # –î–æ–±–∞–≤—å —Å–≤–æ–π –∫–ª—é—á
        base_url="https://api.proxyapi.ru/openai/v1"
    )
themes = themer.get_themes(segments_short, audio_path=VIDEO_PATH)

merger = UnifiedSceneMerger("video.mp4")
final_scenes = merger.run()