{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2739479f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\d.krasnov.kulturauao\\Projects\\My_projects\\Hachakaton\\kion_practice\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\d.krasnov.kulturauao\\Projects\\My_projects\\Hachakaton\\kion_practice\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# –ò–º–ø–æ—Ä—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –º–æ–¥—É–ª–µ–π\n",
    "import os\n",
    "import pickle\n",
    "from classes.audio_analyzer import AudioSceneAnalyzer\n",
    "from classes.video_analyzer_complex import VideoPipeline\n",
    "from classes.scene_enricher import SceneEnricher\n",
    "from classes.openai import OpenAIThemer\n",
    "from classes.simple_transcriptor import ASRProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7abf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = \"videos/Video_01.mp4\"          # –ü—É—Ç—å –∫ –≤–∏–¥–µ–æ—Ñ–∞–π–ª—É\n",
    "AUDIO_PATH = \"temp/temp_audio.wav\"           # –í—Ä–µ–º–µ–Ω–Ω—ã–π WAV\n",
    "OUTPUT_DIR = \"output_files/\"\n",
    "TEMP_DIR = \"temp/\"         # –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "CACHE_PATH = os.path.join(TEMP_DIR, \"results.pkl\")\n",
    "MIN_SCENE_LENGTH = 2.0                  # –ú–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã\n",
    "MAX_SCENE_LENGTH = 300.0                # –ú–∞–∫—Å. –¥–æ–ø—É—Å—Ç–∏–º–∞—è –¥–ª–∏–Ω–∞ —Å—Ü–µ–Ω—ã (5 –º–∏–Ω)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20690d6793c8bd80",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –≤–∏–¥–µ–æ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å—Ü–µ–Ω\n",
    "\n",
    "### –í —ç—Ç–æ–π —è—á–µ–π–∫–µ –º—ã —Å–æ–∑–¥–∞—ë–º –æ–±—ä–µ–∫—Ç VideoAnalyzer –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∏–¥–µ–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ü–µ–Ω—ã –∏ –ª–∏—Ü, –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –∫ —Ç—Ä–µ–∫–∞–º.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da107042",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CACHE_PATH):\n",
    "    print(\"üîÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ –∫–µ—à–∞...\")\n",
    "    with open(CACHE_PATH, \"rb\") as f:\n",
    "        scenes, track_faces, tracking_frames, track_id_to_person = pickle.load(f)\n",
    "\n",
    "else:\n",
    "    pipeline = VideoPipeline()\n",
    "    scenes, track_faces, tracking_frames, track_id_to_person = pipeline.run(VIDEO_PATH, OUTPUT_DIR)\n",
    "\n",
    "    with open(CACHE_PATH, \"wb\") as f:\n",
    "        pickle.dump((scenes, track_faces, tracking_frames, track_id_to_person), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ae0ab96893355",
   "metadata": {},
   "source": [
    "# –ê–Ω–∞–ª–∏–∑ –∞—É–¥–∏–æ: —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –∏ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å\n",
    "\n",
    "### –ò—Å–ø–æ–ª—å–∑—É–µ–º AudioAnalyzer –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—á–∏ –∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∑–≤—É–∫–∞ (—ç–Ω–µ—Ä–≥–∏–∏)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b273c0e8880d4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T18:04:30.637952Z",
     "start_time": "2025-07-21T18:02:12.296697Z"
    }
   },
   "outputs": [],
   "source": [
    "splitter = AudioSceneAnalyzer(\n",
    "    video_path=VIDEO_PATH,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    whisper_model_size='small'\n",
    ")\n",
    "splitter.run(sensitivity=0.88, min_scene_duration=2.0)\n",
    "segments = splitter.transcribe_audio()\n",
    "energy = splitter.detect_audio_activity(frame_duration=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123b0a082d3914dc",
   "metadata": {},
   "source": [
    "# –û–±–æ–≥–∞—â–µ–Ω–∏–µ —Å—Ü–µ–Ω —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–∞–º–∏ –∏ –∞—É–¥–∏–æ —ç–Ω–µ—Ä–≥–∏–µ–π\n",
    "\n",
    "### –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—Ü–µ–Ω–∞–º —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ —É—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –∑–≤—É–∫–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ca4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enricher = SceneEnricher(segments, energy)\n",
    "scenes_final = enricher.run(scenes, track_id_to_person)\n",
    "#scenes_final = enricher.run(scene_data, track_id_to_person, print_report=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d81463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr = ASRProcessor()\n",
    "segments_short = asr.process(VIDEO_PATH)\n",
    "themer = OpenAIThemer(\n",
    "        api_key=\"\",  # –î–æ–±–∞–≤—å —Å–≤–æ–π –∫–ª—é—á\n",
    "        base_url=\"https://api.proxyapi.ru/openai/v1\"\n",
    "    )\n",
    "themes = themer.get_themes(segments_short, audio_path=VIDEO_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
