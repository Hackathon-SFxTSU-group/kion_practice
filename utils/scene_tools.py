import re
from sentence_transformers import util
import numpy as np
from sklearn.cluster import DBSCAN
import os
import json

def enrich_scenes_with_characters(scene_data, track_id_to_person):
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –≤ –∫–∞–∂–¥—É—é —Å—Ü–µ–Ω—É –ø–æ track_id ‚Üí person mapping
    for scene in scene_data:
        scene['characters'] = list({
            track_id_to_person.get(tid)  # –ü–æ–ª—É—á–∞–µ–º –∏–º—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –ø–æ track_id
            for tid in scene['track_ids']
            if tid in track_id_to_person
        })
    return scene_data


def normalize_text(t):
    # –ü—Ä–∏–≤–æ–¥–∏–º —Ç–µ–∫—Å—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    return re.sub(r'[^\w\s]', '', t.lower()).strip()


def get_text_score(a, b, sentenceTransformer):
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤
    a, b = normalize_text(a), normalize_text(b)
    if not a or not b:
        return 0.0
    a_embed = sentenceTransformer.encode(a, convert_to_tensor=True)
    b_embed = sentenceTransformer.encode(b, convert_to_tensor=True)
    return float(util.cos_sim(a_embed, b_embed)[0][0])


def jaccard_similarity(a, b):
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ñ–∞–∫–∫–∞—Ä–∞ –¥–ª—è –¥–≤—É—Ö –º–Ω–æ–∂–µ—Å—Ç–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π)
    set_a, set_b = set(a), set(b)
    intersection = set_a & set_b
    union = set_a | set_b
    return len(intersection) / len(union) if union else 0


def group_semantic_scenes(
        scene_data,
        sentenceTransformer,
        char_thresh=0.5,
        text_thresh=0.55,
        audio_thresh=0.02
    ):
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å—Ü–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ—Ö–æ–∂–∏ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º, —Ç–µ–∫—Å—Ç—É –∏ –∞—É–¥–∏–æ –ø–æ –ø–æ—Ä–æ–≥–∞–º
    grouped = []
    buffer = [scene_data[0]]

    prev_text = scene_data[0]['transcript']

    for curr in scene_data[1:]:
        last = buffer[-1]
        curr_ids = get_identities(curr)
        last_ids = get_identities(last)
        char_score = jaccard_similarity(curr_ids, last_ids)  # –°—Ö–æ–¥—Å—Ç–≤–æ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π

        curr_text = curr['transcript']
        text_score = get_text_score(prev_text, curr_text, sentenceTransformer)  # –°—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞

        audio_diff = abs(curr['avg_rms'] - last['avg_rms'])  # –†–∞–∑–Ω–∏—Ü–∞ –ø–æ –≥—Ä–æ–º–∫–æ—Å—Ç–∏

        if char_score >= char_thresh and text_score >= text_thresh and audio_diff <= audio_thresh:
            # –ï—Å–ª–∏ —Å—Ü–µ–Ω—ã –ø–æ—Ö–æ–∂–∏, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç
            buffer.append(curr)
            prev_text = " ".join([prev_text, curr_text])
        else:
            # –ò–Ω–∞—á–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é –≥—Ä—É–ø–ø—É –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é
            grouped.append({
                'start': buffer[0]['start'],
                'end': buffer[-1]['end'],
                'characters': sorted(set().union(*(get_identities(b) for b in buffer))),
                'transcript': " ".join(b['transcript'] for b in buffer),
                'avg_rms': float(np.mean([b['avg_rms'] for b in buffer]))
            })
            buffer = [curr]
            prev_text = curr['transcript']

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≥—Ä—É–ø–ø—É —Å—Ü–µ–Ω
    if buffer:
        grouped.append({
            'start': buffer[0]['start'],
            'end': buffer[-1]['end'],
            'characters': sorted(set().union(*(get_identities(b) for b in buffer))),
            'transcript': " ".join(b['transcript'] for b in buffer),
            'avg_rms': float(np.mean([b['avg_rms'] for b in buffer]))
        })

    return grouped

def cluster_scenes_with_time_windows(scenes, sentenceTransformer, window_size=60, eps=0.45, min_samples=2):
    max_time = max(s['end'] for s in scenes)
    scenes_sorted = sorted(scenes, key=lambda x: x['start'])
    chapters = []

    start_window = 0
    while start_window < max_time:
        end_window = start_window + window_size
        window_scenes = [s for s in scenes_sorted if s['start'] >= start_window and s['start'] < end_window]
        if not window_scenes:
            start_window = end_window
            continue

        transcripts = [s['transcript'] for s in window_scenes]
        embeddings = sentenceTransformer.encode(transcripts, convert_to_tensor=False, normalize_embeddings=True)
        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
        labels = clustering.fit_predict(embeddings)

        clusters = {}
        for label, scene in zip(labels, window_scenes):
            if label == -1:
                label = f"scene_{scene['start']:.2f}"
            if label not in clusters:
                clusters[label] = {
                    "start": scene["start"],
                    "end": scene["end"],
                    "characters": set(scene["characters"]),
                    "transcripts": [scene["transcript"]],
                    "avg_rms": [scene["avg_rms"]],
                }
            else:
                clusters[label]["end"] = max(clusters[label]["end"], scene["end"])
                clusters[label]["characters"].update(scene["characters"])
                clusters[label]["transcripts"].append(scene["transcript"])
                clusters[label]["avg_rms"].append(scene["avg_rms"])

        for cluster in clusters.values():
            chapters.append({
                "start": cluster["start"],
                "end": cluster["end"],
                "characters": sorted(list(cluster["characters"])),
                "transcript": " ".join(cluster["transcripts"]),
                "avg_rms": float(np.mean(cluster["avg_rms"]))
            })

        start_window = end_window

    chapters = sorted(chapters, key=lambda x: x['start'])
    return chapters


def resolve_time_overlaps(chapters):
    """
    –£–±–∏—Ä–∞–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤ —Å–ø–∏—Å–∫–µ —Å—Ü–µ–Ω/–≥–ª–∞–≤.
    –ï—Å–ª–∏ —Å—Ü–µ–Ω—ã –Ω–∞–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è, –ø–æ–¥—Ä–µ–∑–∞–µ—Ç –Ω–∞—á–∞–ª–æ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ü–µ–Ω—ã.
    """
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ start
    chapters.sort(key=lambda x: x['start'])

    resolved = []
    prev_end = 0
    for ch in chapters:
        start = max(ch['start'], prev_end)
        end = ch['end']
        if start >= end:
            # —Å—Ü–µ–Ω–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–∫—Ä—ã—Ç–∞ ‚Äî –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–ª–∏ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å
            continue
        resolved.append({
            **ch,
            'start': start,
            'end': end
        })
        prev_end = end
    return resolved


def enrich_scenes_with_audio(scenes, segments, energy, frame_duration=1.0):
    # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—Ü–µ–Ω–∞–º —Ç–µ–∫—Å—Ç –∏ –∞—É–¥–∏–æ-—ç–Ω–µ—Ä–≥–∏—é –∏–∑ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –∏ –º–∞—Å—Å–∏–≤–∞ —ç–Ω–µ—Ä–≥–∏–∏

    for scene in scenes:
        start, end = scene["start"], scene["end"]

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫—É—Å–æ—á–∫–∏ —Ç–µ–∫—Å—Ç–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ü–µ–Ω–µ
        scene_texts = [
            clip_text_to_scene(seg, start, end)
            for seg in segments
            if not (seg["end"] < start or seg["start"] > end)
        ]
        scene["transcript"] = " ".join(filter(None, scene_texts)).strip()

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω—é—é —ç–Ω–µ—Ä–≥–∏—é –∞—É–¥–∏–æ –¥–ª—è —Å—Ü–µ–Ω—ã
        start_idx = max(0, int(start // frame_duration))
        end_idx = min(len(energy) - 1, int(end // frame_duration))
        scene_energy = energy[start_idx:end_idx + 1] if end_idx >= start_idx else []
        scene["avg_rms"] = float(np.mean(scene_energy)) if scene_energy else 0.0

    return scenes


def clean_and_merge_short_scenes(scenes, min_duration=2.0, min_words=3):
    # –£–±–∏—Ä–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ –º–∞–ª–æ—Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Ü–µ–Ω—ã, —Å–ª–∏–≤–∞—è –∏—Ö —Å —Å–æ—Å–µ–¥–Ω–∏–º–∏
    cleaned = []
    buffer = None

    for scene in scenes:
        duration = scene["end"] - scene["start"]
        word_count = len(scene["transcript"].strip().split())

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ü–µ–Ω—ã —Å –º–∞–ª–æ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
        if word_count < min_words or duration < 0.5:
            if cleaned:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∞—è —Å—Ü–µ–Ω–∞, —Å–ª–∏–≤–∞–µ–º —Å –Ω–µ–π
                cleaned[-1] = merge_scene(cleaned[-1], scene)
            continue

        if duration < min_duration:
            if cleaned:
                # –°–ª–∏–≤–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é —Å—Ü–µ–Ω—É —Å –ø—Ä–µ–¥—ã–¥—É—â–µ–π
                cleaned[-1] = merge_scene(cleaned[-1], scene)
            else:
                buffer = scene
        else:
            cleaned.append(scene)

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞—Ç–æ–∫ –∏–∑ –±—É—Ñ–µ—Ä–∞
    if buffer:
        if cleaned:
            cleaned[-1] = merge_scene(cleaned[-1], buffer)
        else:
            cleaned.append(buffer)

    return cleaned


def print_scenes_formatted(scenes):
    # –í—ã–≤–æ–¥–∏—Ç –∫—Ä–∞—Å–∏–≤–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ —Å—Ü–µ–Ω–∞–º
    for i, scene in enumerate(scenes):
        print(f"\nüé¨ –°—Ü–µ–Ω–∞ {i + 1}")
        print(f"‚è± –í—Ä–µ–º—è: {scene['start']:.2f} ‚Äì {scene['end']:.2f} —Å–µ–∫")
        print(f"üßç –ü–µ—Ä—Å–æ–Ω–∞–∂–∏ (characters): {', '.join(scene['characters'])}")
        print(f"üîä –°—Ä–µ–¥–Ω—è—è –≥—Ä–æ–º–∫–æ—Å—Ç—å: {scene['avg_rms']:.4f}")
        print(f"üó£ –†–µ–ø–ª–∏–∫–∞:\n\"{scene['transcript'].strip()}\"")
        print("-" * 60)

def save_scenes_report_to_json(scenes):
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ reports
    os.makedirs("reports", exist_ok=True)

    base_name = "scenes_enriched"

    output_path = os.path.join("reports", f"{base_name}_report.json")

    # –§–æ—Ä–º–∏—Ä—É–µ–º "—Å—É—Ö–æ–π" –æ—Ç—á–µ—Ç
    scene_list = []
    for i, scene in enumerate(scenes):
        scene_info = {
            "scene_id": i + 1,
            "start_time_sec": round(scene["start"], 2),
            "end_time_sec": round(scene["end"], 2),
            "characters": scene.get("characters", []),
            "avg_rms": round(scene.get("avg_rms", 0.0), 4),
            "transcript": scene.get("transcript", "").strip()
        }
        scene_list.append(scene_info)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ JSON
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(scene_list, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ JSON-–æ—Ç—á—ë—Ç –ø–æ —Å—Ü–µ–Ω–∞–º —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")

def merge_scene(a, b):
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–≤–µ —Å—Ü–µ–Ω—ã –≤ –æ–¥–Ω—É
        return {
            "start": a["start"],
            "end": b["end"],
            "characters": sorted(set(a.get("characters", []) + b.get("characters", []))),
            "transcript": " ".join([a["transcript"], b["transcript"]]).strip(),
            "avg_rms": float(np.mean([a["avg_rms"], b["avg_rms"]]))
        }

def clip_text_to_scene(seg, start, end):
        # –í—ã—Ä–µ–∑–∞–µ–º —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ —Å–µ–≥–º–µ–Ω—Ç–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Å—Ü–µ–Ω–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        seg_start, seg_end = seg["start"], seg["end"]
        overlap_start = max(start, seg_start)
        overlap_end = min(end, seg_end)
        overlap_dur = max(0.0, overlap_end - overlap_start)
        total_dur = seg_end - seg_start

        if overlap_dur == 0 or total_dur == 0:
            return ""

        words = seg["text"].strip().split()
        if len(words) <= 2:
            return seg["text"].strip()

        ratio = overlap_dur / total_dur
        count = max(1, int(len(words) * ratio))
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–µ –∏–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–æ–∑–∏—Ü–∏–∏ —Å—Ü–µ–Ω—ã –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
        return " ".join(words[:count]) if overlap_start == seg_start else " ".join(words[-count:])

def get_identities(scene):
    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏–ª–∏ track_ids –¥–ª—è —Å—Ü–µ–Ω—ã
    return scene.get('characters', []) or scene.get('track_ids', [])